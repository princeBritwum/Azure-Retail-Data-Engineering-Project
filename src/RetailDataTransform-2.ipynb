{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": [],
              "state": "session_starting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "session_starting",
              "queued_time": "2024-08-21T03:48:04.7581762Z",
              "session_start_time": "2024-08-21T03:48:04.8130814Z",
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "c7f2121a-436f-4c18-b138-e2397a4e73fa"
            },
            "text/plain": "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "''' orderSchema = StructType([\n",
        "    StructField(\"SalesOrderNumber\", StringType()),\n",
        "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
        "    StructField(\"OrderDate\", DateType()),\n",
        "    StructField(\"CustomerName\", StringType()),\n",
        "    StructField(\"Email\", StringType()),\n",
        "    StructField(\"Item\", StringType()),\n",
        "    StructField(\"Quantity\", IntegerType()),\n",
        "    StructField(\"UnitPrice\", FloatType()),\n",
        "    StructField(\"Tax\", FloatType())\n",
        "    ]) '''\n",
        "    \n",
        "df = spark.read.load('abfss://files@datalakecpvl0xw.dfs.core.windows.net/sales_data/dbo.DimFactRetail.txt', \n",
        "format='CSV',header=True\n",
        ")\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": null,
              "statement_ids": null,
              "state": "waiting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "waiting",
              "queued_time": "2024-08-21T03:48:04.7590872Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "222d6393-d83a-445f-aca5-4302f3118768"
            },
            "text/plain": "StatementMeta(, , , Waiting, , Waiting)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\n",
        "from pyspark.sql.functions import * #year,month, col, split\n",
        "RetailSales = spark.read.load(\"abfss://files@datalakecpvl0xw.dfs.core.windows.net/sales_data/dbo.DimFactRetail.txt\",\n",
        "format = 'CSV',header=True,inferSchema=True)\n",
        "\n",
        "display(RetailSales.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": null,
              "statement_ids": null,
              "state": "waiting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "waiting",
              "queued_time": "2024-08-21T03:48:04.76001Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "4483e0fc-8a53-44b3-8c85-bf4490bc9a44"
            },
            "text/plain": "StatementMeta(, , , Waiting, , Waiting)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql import functions as sf\n",
        "#from pyspark.sql import functions as sf\n",
        "transform_df = RetailSales.withColumn('FullName',\n",
        "                                     sf.concat_ws(' ',sf.col('FirstName'),sf.col('MiddleName'),sf.col('LastName')))\n",
        "display(transform_df.limit(10));\n",
        "transform_df = transform_df.drop('FirstName','MiddleName','LastName')\n",
        "transform_df.write.mode('overwrite').parquet('/retaildata/destination/TransformedRetailData/FactRetail.parquet')\n",
        "print (\"Transformed data saved!\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": null,
              "statement_ids": null,
              "state": "waiting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "waiting",
              "queued_time": "2024-08-21T03:48:04.7612945Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "4bab4e3c-2419-49d9-9d4b-f18cdc312b15"
            },
            "text/plain": "StatementMeta(, , , Waiting, , Waiting)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "partitioned_df = transform_df.withColumn('Year',year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n",
        "display(partitioned_df.limit(10))\n",
        "partitionedDate_df = partitioned_df.write.partitionBy('Year').mode('overwrite').parquet('/retaildata/destination/PartitionedRetailData')\n",
        "print(\"partioned data saved\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": null,
              "statement_ids": null,
              "state": "waiting",
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "normalized_state": "waiting",
              "queued_time": "2024-08-21T03:48:04.7625676Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "09374d44-2595-4511-bad4-444de7d1fbc1"
            },
            "text/plain": "StatementMeta(, , , Waiting, , Waiting)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "FactRetail2010 = spark.read.load('/retaildata/destination/PartitionedRetailData/Year=2010')\n",
        "OderedFactRetail2010 = FactRetail2010.orderBy(FactRetail2010.SalesAmount.desc())\n",
        "display(OderedFactRetail2010.limit(10))"
      ]
    }
  ]
}